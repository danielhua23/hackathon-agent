[{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton operator code implements a matrix-vector multiplication kernel, `mv_kernel`, designed for efficient execution on NVIDIA GPUs. It leverages Triton's Just-In-Time (JIT) compilation and auto-tuning features. The kernel function multiplies a matrix `A` of size N x M with a vector `B` of size M, resulting in an output vector `C` of size N. The kernel is parameterized by constants `BLOCK_N` and `BLOCK_M` which determine the tile sizes for the computation, allowing for optimal parallel processing. Within `mv_kernel`, a loop iterates over the matrix A in blocks of `BLOCK_M` to load sub-matrices and the corresponding block from vector B, performing element-wise multiplication and accumulation in shared memory. The partial results are then reduced along the rows and stored in the output vector `C`. The `mv` function wraps this kernel execution, asserting dimension compatibility and managing CUDA resources for launching the kernel with calculated grid dimensions.\n            ", "label": null, "file": "matrix_vector_multip.py", "test_code": "\n\n\ndef test_mv():\n    # \u6d4b\u8bd5\u7528\u4f8b 2: 4x3 \u77e9\u9635\u4e0e 3x1 \u5411\u91cf\u76f8\u4e58\n    A = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]], device='cuda')\n    B = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n    triton_result_2 = mv(A, B)\n\n    # \u6d4b\u8bd5\u7528\u4f8b 3: 32x16 \u77e9\u9635\u4e0e 16x1 \u5411\u91cf\u76f8\u4e58\n    A = torch.randn(32, 16, device='cuda')\n    B = torch.randn(16, device='cuda')\n    triton_result_3 = mv(A, B)\n\n    return {\n        \"test_case_2\": triton_result_2,\n        \"test_case_3\": triton_result_3,\n    }\n\nresult_gold = test_mv()\n", "predict": "\nimport triton\nimport triton.language as tl\nimport torch\n\n\n@triton.jit\ndef mv_kernel(A, B, C, M, N, stride_am, stride_an, stride_b, stride_c, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    pid_n = tl.program_id(0)\n    offs_n = pid_n * BLOCK_M + tl.arange(0, BLOCK_M)\n    mask_n = offs_n < M\n    acc = tl.zeros([BLOCK_M], dtype=tl.float32)\n    for k_off in tl.static_range(0, (N + BLOCK_N - 1) // BLOCK_N):\n        offs_k = k_off * BLOCK_N + tl.arange(0, BLOCK_N)\n        mask_k = offs_k < N\n        a_idx = A + (offs_n[:, None] * stride_am + offs_k[None, :] * stride_an)\n        b_idx = B + offs_k * stride_b\n        a_blk = tl.load(a_idx, mask=mask_n[:, None] & mask_k[None, :], other=0.0)\n        b_blk = tl.load(b_idx, mask=mask_k, other=0.0)\n        acc += tl.sum(a_blk * b_blk[None, :], 1)\n    c_idx = C + offs_n * stride_c\n    tl.store(c_idx, acc.to(C.dtype.element_ty), mask=mask_n)\n\n\ndef mv(A: torch.Tensor, B: torch.Tensor, out=None):\n    assert A.dim() == 2 and B.dim() == 1, \"A must be 2-D and B must be 1-D\"\n    M, N = A.shape\n    assert B.size(0) == N, \"Size mismatch\"\n    if out is None:\n        out = torch.empty(M, dtype=A.dtype, device=A.device)\n    else:\n        assert out.dtype == A.dtype and out.size(0) == M\n    BLOCK_M = 4\n    BLOCK_N = 64\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]),)\n    mv_kernel[grid](\n        A, B, out, M, N,\n        A.stride(0), A.stride(1),\n        B.stride(0),\n        out.stride(0),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,\n    )\n    return out\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided Triton kernel, `matmul_kernel`, is a specialized GPU matrix multiplication operation. \n        It employs a blocked tiling strategy for efficient computation of the result matrix `c` from input matrices `a` and `b`. \n        Within this kernel, operations are parallelized across blocks defined by BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K. \n        These blocks allow the kernel to load sub-matrices, perform computations, and manage memory more efficiently.\n\n        The kernel begins by computing indices for thread execution, segmenting the operation across various program IDs derived from the grid dimensions. \n        For each thread block, it computes offsets `offs_am`, `offs_bn`, and `offs_k` to read data from the input matrices.\n\n        In a loop iterating over slices of the K dimension, sub-matrices are loaded using `tl.load` with masks to handle boundary conditions. \n        These matrices are then multiplied using `tl.dot`, accumulating results in a local accumulator. \n        Memory access patterns are optimized using `tl.max_contiguous` and `tl.multiple_of` to align data in cache-friendly ways.\n\n        The function finally writes the accumulated results to the output matrix `c`, with care taken to respect bounds and using conditional storage via `tl.store`.\n\n        The `matmul` function wraps this kernel, preparing inputs and meta-parameters based on the matrix data types and dimensions. \n        It enforces input compatibility, establishes execution grid dimensions, and sets device memory for output. \n        Configuration parameters such as BLOCK_SIZE_M, num_stages, and num_warps are determined per data type, \n        ensuring optimal kernel execution tailored for either float16 or Triton's experimental float8 types.\n    ", "label": null, "file": "triton_matmul.py", "test_code": "\n\n\nimport torch\n\n# Test for matmul\ndef test_matmul():\n    results = {}\n    M, K, N = 256, 128, 256\n\n    # Test case 1: torch.float16\n    a = torch.randn((M, K), dtype=torch.float16, device='cuda')\n    b = torch.randn((K, N), dtype=torch.float16, device='cuda')\n    c = matmul(a, b)\n    results['test_case_1'] = c\n\n    return results\n\n# Run all tests\nresult_gold = test_matmul()", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        # SAFE configs\n        triton.Config({'BLOCK_M': 64,  'BLOCK_N': 64,  'BLOCK_K': 32, 'GROUP_M': 4}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64,  'BLOCK_K': 32, 'GROUP_M': 4}, num_stages=1, num_warps=4),\n        # AGGRESSIVE configs\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 64, 'GROUP_M': 8}, num_stages=3, num_warps=16),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) % num_pid_n\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k  = tl.arange(0, BLOCK_K)\n\n    A_ptrs  = A_ptr + (offs_am[:, None]*stride_am + offs_k[None, :]*stride_ak)\n    B_ptrs  = B_ptr + (offs_k[:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n    C_ptrs  = C_ptr + offs_am[:, None]*stride_cm + offs_bn[None, :]*stride_cn\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in tl.static_range(0, tl.cdiv(K, BLOCK_K)):\n        k_offs = (k * BLOCK_K) + offs_k\n        mask_k = k_offs < K\n        mask_am = offs_am < M\n        mask_bn = offs_bn < N\n\n        a = tl.load(A_ptrs, mask=mask_k[None,] & mask_am[:, None], other=0.0)\n        b = tl.load(B_ptrs, mask=mask_k[:, None] & mask_bn[None, :], other=0.0)\n        accumulator += tl.dot(a, b).to(tl.float32)\n\n        A_ptrs += BLOCK_K * stride_ak\n        B_ptrs += BLOCK_K * stride_bk\n\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n\n    tl.store(C_ptrs, accumulator, mask=c_mask)\n\n\ndef matmul(a: torch.Tensor, b: torch.Tensor, out: torch.Tensor = None, eps: float = 1e-6):\n    assert a.ndim == 2 and b.ndim == 2\n    assert a.shape[1] == b.shape[0]\n    assert a.dtype == b.dtype, \"Mixed types not handled\"\n    assert a.device == b.device\n    assert a.is_contiguous() and b.is_contiguous(), \"Use .contiguous()\"\n\n    M, K = a.shape\n    K2, N = b.shape\n    assert K == K2\n    if out is None:\n        out = torch.empty((M, N), dtype=a.dtype, device=a.device)\n    else:\n        assert out.shape == (M, N) and out.dtype == a.dtype and out.is_contiguous()\n\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n\n    matmul_kernel[grid](\n        a, b, out,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        out.stride(0), out.stride(1),\n    )\n    return out\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton-accelerated function embedding_kernel is specialized for extracting and storing embedding vectors from a weight matrix for a sequence of token IDs. It uses program IDs to determine processing offsets and handles iteration over sequences with BLOCK_N and BLOCK_NN stride sizes. For each sequence, it computes token IDs and uses masks to ensure only valid data is loaded and processed. The weight matrix is addressed using a combination of token IDs and dimension offsets, facilitated by the stride of the weight tensor. The processed vectors are then stored into the 'out' tensor using calculated strides and masks, ensuring each output sequence position receives the correct embedding vector. The wrapping function, embedding, configures and invokes the kernel with appropriate grid settings, aligning BLOCK_DMODEL to the next power of two based on weight dimensions and leveraging constant memory settings to optimize the embedding extraction process.\n            ", "label": null, "file": "embedding_triton_kernel.py", "test_code": "\n\n\nimport torch\n\ndef test_embedding():\n    # \u53c2\u6570\u5b9a\u4e49\n    vocab_size = 1000         # \u8bcd\u6c47\u8868\u5927\u5c0f\n    embedding_dim = 512       # \u5d4c\u5165\u7ef4\u5ea6\n    sequence_length = 128     # \u8f93\u5165\u5e8f\u5217\u957f\u5ea6\n    vob_start_id = 10         # \u8bcd\u6c47\u8868\u8d77\u59cb ID\n    vob_end_id = 1000         # \u8bcd\u6c47\u8868\u7ed3\u675f ID\n\n    # \u521b\u5efa\u6d4b\u8bd5\u8f93\u5165\u5f20\u91cf\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    weight = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    out = torch.zeros(\n        sequence_length, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n\n    # \u8c03\u7528\u5d4c\u5165\u51fd\u6570\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n\n    # \u4fdd\u5b58\u7ed3\u679c\n    results = {}\n    results['test_case_1'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u8f93\u5165\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_2'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u8bcd\u6c47\u8868\u8303\u56f4\n    vob_start_id = 0\n    vob_end_id = 500\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_3'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u5d4c\u5165\u7ef4\u5ea6\n    embedding_dim = 256\n    weight = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    out = torch.zeros(\n        sequence_length, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_4'] = out.clone()\n\n    return results\n\nresult_gold = test_embedding()\n", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef embedding_kernel(\n    out,               # ptr\n    weight,            # ptr\n    indices,           # ptr\n    seq_len,\n    hidden,\n    stride_out_s,       # tl.constexpr\n    stride_out_d,       # tl.constexpr\n    stride_weight_v,    # tl.constexpr\n    stride_weight_d,    # tl.constexpr\n    vob_start_id,       # int32  (constexpr if always present)\n    vob_end_id,         # int32  (constexpr if always present)\n    BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n    pid_s = tl.program_id(0)           # tile along sequence\n    pid_d = tl.program_id(1)           # tile along hidden dimension\n\n    offs_n = pid_s * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = pid_d * BLOCK_DMODEL + tl.arange(0, BLOCK_DMODEL)\n\n    mask_n = offs_n < seq_len\n    mask_d = offs_d < hidden\n    mask_nd = mask_n[:, None] & mask_d[None, :]\n\n    # fetch ids  (clamp after mask)\n    ids = tl.load(indices + offs_n, mask=mask_n, other=0).to(tl.int32)\n\n    # build valid mask for embeddings\n    valid_id_mask = (ids >= vob_start_id) & (ids < vob_end_id)\n    valid_word_idx = ids - vob_start_id                   # 0-based inside table\n    masked_valid = valid_id_mask & mask_n                 # update mask along sequence\n\n    # prepare broadcasted indices & dims\n    w_idx_n = valid_word_idx[None, :]                     # (1, BLOCK_N)\n    w_idx_d = offs_d[None, :]                             # (1, BLOCK_DMODEL)\n    w_mask_nd = masked_valid[None, :] & mask_d[None, :]   # broadcast to (BLOCK_DMODEL, BLOCK_N)\n\n    w_ptr = weight + w_idx_n * stride_weight_v + w_idx_d.T * stride_weight_d\n    embed = tl.load(w_ptr, mask=w_mask_nd, other=0.0)\n\n    # store to out\n    o_ptr = out + offs_n[:, None] * stride_out_s + offs_d[None, :] * stride_out_d\n    tl.store(o_ptr, embed, mask=mask_nd)\n\n\ndef embedding(\n    weight: torch.Tensor,\n    indices: torch.Tensor,\n    out: torch.Tensor | None = None,\n    vob_start_id: int = 0,\n    vob_end_id: int | None = None,\n) -> torch.Tensor:\n    assert weight.device.type == \"cuda\"\n    assert indices.device == weight.device\n    assert weight.dim() == 2 and indices.dim() == 1\n\n    vob_end_id = weight.size(0) if vob_end_id is None else vob_end_id\n    seq_len = indices.size(0)\n    hidden = weight.size(1)\n\n    if out is None:\n        out = torch.empty((seq_len, hidden), dtype=weight.dtype, device=weight.device)\n    else:\n        assert out.shape == (seq_len, hidden)\n        assert out.dtype == weight.dtype\n        assert out.device == indices.device\n\n    BLOCK_DMODEL = triton.next_power_of_2(hidden)\n    BLOCK_N = 128     # tile along sequence\n\n    grid = (triton.cdiv(seq_len, BLOCK_N), triton.cdiv(hidden, BLOCK_DMODEL))\n\n    embedding_kernel[grid](\n        out,\n        weight,\n        indices,\n        seq_len,\n        hidden,\n        out.stride(0),\n        out.stride(1),\n        weight.stride(0),\n        weight.stride(1),\n        vob_start_id,\n        vob_end_id,\n        BLOCK_N=BLOCK_N,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n    )\n\n    return out\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines a Triton-based kernel for matrix multiplication of INT4 quantized weights and provides Python functions to handle the quantization and dequantization processes. \n        The 'matmul_kernel' function is a Triton kernel using @triton.jit which performs matrix multiplication. It processes the input matrices in tiles defined by BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K, and uses a loop to iterate over the K dimension to accumulate results in fp32 precision, subsequently stored in the output buffer, potentially employing atomic add for SPLIT_K > 1.\n        The kernel is set up with a wide variety of configurations, allowing Triton's autotuning capabilities to select optimal parameters based on matrix dimensions M, N, and K. It utilizes quantized INT4 weights, reconstructing them using scales and zero points to compute matrix products accurately. \n        The Python function 'matmul_dequantize_int4_s2' serves as an interface to this kernel, preparing data and launching the computation on specified grid dimensions.\n        The function 'quantize_int4' converts a floating-point weight matrix into INT4 format, organizing the data by packing 8 INT4 values into one INT32 and calculating scaling factors and zero points for each group of elements. \n        The helper function 'unpack_int4' is intended for testing; it unpacks the INT4 matrix back into a floating-point format to verify the quantization process.\n    ", "label": null, "file": "int4_matmul.py", "test_code": "\n\n\ndef test_correct_int4_s2(M=32, K=4096, N=4096):\n    group_size = 128\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    int_b, b_scale, b_zero_point, _ = quantize_int4(b, group_size=group_size)\n    \n    # Test case\n    triton_output = matmul_dequantize_int4_s2(a, int_b, b_scale, b_zero_point, group_size)\n    \n    results = {\n        \"test_case_1\": triton_output\n    }\n    \n    return results\n\nresult_gold = test_correct_int4_s2()\n", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\ndef quantize_int4(weight: torch.Tensor, group_size: int = 128) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    assert weight.dtype == torch.float16 or weight.dtype == torch.float32\n    K, N = weight.shape\n    assert K % group_size == 0\n    num_groups = K // group_size\n    weight_int = torch.empty((K, N // 8), dtype=torch.int32, device=weight.device)\n\n    scales = torch.empty((num_groups, N), dtype=weight.dtype, device=weight.device)\n    zeros = torch.empty((num_groups, N // 8), dtype=torch.int32, device=weight.device)\n\n    for g in range(num_groups):\n        start = g * group_size\n        end = start + group_size\n        group = weight[start:end, :]\n        min_val = group.min(dim=0)[0]\n        max_val = group.max(dim=0)[0]\n        scale = (max_val - min_val) / 15\n        scale = torch.clamp(scale, min=1e-8)\n        zero = -min_val / scale\n        zero_int = torch.clamp(zero.round(), 0, 15).to(torch.int32)\n        q = torch.clamp((group / scale + zero_int).round(), 0, 15).to(torch.int32)\n\n        packed = torch.zeros((group_size, N // 8), dtype=torch.int32, device=weight.device)\n        for i in range(8):\n            packed |= q[:, i * (N // 8):(i + 1) * (N // 8)] << (4 * i)\n\n        weight_int[start:end, :] = packed\n        scales[g, :] = scale\n        zeros[g, :] = torch.sum(zero_int.view(-1, N // 8) << (4 * torch.arange(8, device=weight.device, dtype=torch.int32)[None, :]), dim=1)\n\n    return weight_int, scales, zeros\n\ndef unpack_int4(qweight: torch.Tensor, scales: torch.Tensor, zeros: torch.Tensor, group_size: int = 128) -> torch.Tensor:\n    K, N_pack = qweight.shape\n    N = N_pack * 8\n    num_groups = K // group_size\n    weight_fp = torch.empty((K, N), dtype=scales.dtype, device=qweight.device)\n\n    for g in range(num_groups):\n        start = g * group_size\n        end = start + group_size\n        group_q = qweight[start:end, :]  # (GS, N//8)\n        zp_group = zeros[g, :]  # (N//8,)\n        scale_group = scales[g, :]  # (N,)\n\n        unpacked = torch.empty((group_size, N), dtype=torch.int32, device=qweight.device)\n        for i in range(8):\n            mask = 0xF << (4 * i)\n            unpacked[:, i * N_pack:(i + 1) * N_pack] = (group_q & mask) >> (4 * i)\n\n        unpacked = unpacked.view(-1)\n        offset = 0\n        for i in range(8):\n            idx = torch.arange(0, N, device=qweight.device) % 8 == i\n            unpacked_chunk = unpacked[offset:offset + K * (N // 8)]\n            unpacked_chunk = unpacked_chunk.view(K, N // 8)\n            unpacked.view(K, N)[:, idx] = unpacked_chunk\n            offset += K * (N // 8)\n\n        zp_expanded = torch.zeros((N,), dtype=torch.float32, device=qweight.device)\n        offset = 0\n        for i in range(N_pack):\n            for j in range(8):\n                zp_expanded[i * 8 + j] = (zp_group[i] >> (4 * j)) & 0xF\n\n        weight_fp[start:end, :] = (unpacked.view(K, N).float() - zp_expanded[None, :]) * scale_group[None, :]\n    return weight_fp\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'GROUP_M': 4}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'GROUP_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 256, 'BLOCK_K': 64, 'GROUP_M': 8}, num_stages=3, num_warps=16),\n    ],\n    key=['M', 'N', 'K', 'SPLIT_K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, scales_ptr, zeros_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_sm, stride_sn,\n    stride_zm, stride_zn,\n    stride_cm, stride_cn,\n    group_size,\n    SPLIT_K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_m = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_n = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    offs_k = pid_k * BLOCK_K + tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        k_offs = k * BLOCK_K * SPLIT_K + offs_k\n\n        a = tl.load(a_ptrs, mask=k_offs[None, :] < K, other=0.0)\n        b_packed = tl.load(b_ptrs, mask=k_offs[:, None] < K, other=0)\n\n        b_i = (k_offs[:, None] // 8) % (group_size // 8)\n        g = (k_offs[:, None] // group_size) % (K // group_size)\n        scale = tl.load(scales_ptr + g * stride_sm + offs_n[None, :] * stride_sn)\n        zero = tl.load(zeros_ptr + g * stride_zm + (offs_n[None, :] // 8) * stride_zn)\n\n        shifts = (k_offs[:, None] % 8) * 4\n        mask = 0xF << shifts\n        b_int = (b_packed & mask) >> shifts\n        zp = (zero >> ((offs_n[None, :] % 8) * 4)) & 0xF\n        b = (b_int.astype(tl.float32) - zp.astype(tl.float32)) * scale.astype(tl.float32)\n        acc += tl.dot(a, b)\n\n        a_ptrs += BLOCK_K * SPLIT_K * stride_ak\n        b_ptrs += (BLOCK_K * SPLIT_K // 8) * stride_bk\n\n    if SPLIT_K > 1:\n        c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        tl.atomic_add(c_ptrs, acc.astype(tl.float16))\n    else:\n        c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n        tl.store(c_ptrs, acc.astype(tl.float16), mask=mask)\n\ndef matmul_dequantize_int4_s2(a: torch.Tensor, b_q: torch.Tensor, scales: torch.Tensor, zeros: torch.Tensor, group_size: int = 128):\n    orig_device = a.device\n    a = a.to('cuda')\n    b_q = b_q.to('cuda')\n    scales = scales.to('cuda')\n    zeros = zeros.to('cuda')\n\n    assert a.dtype == torch.float16\n    assert b_q.dtype == torch.int32\n    assert a.is_contiguous()\n    assert b_q.is_contiguous()\n    assert scales.is_contiguous()\n    assert zeros.is_contiguous()\n\n    M, K = a.shape\n    K_b, N = b_q.shape\n    assert K == K_b\n    assert K % group_size == 0\n    assert N * 8 == scales.shape[1] and N * 8 == zeros.shape[1] * 8\n    assert group_size > 0\n\n    out = torch.empty((M, N * 8), dtype=torch.float16, device=a.device)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N * 8, META['BLOCK_N']), 1)\n\n    matmul_kernel[grid](\n        a, b_q, scales, zeros, out,\n        M, N * 8, K,\n        a.stride(0), a.stride(1),\n        b_q.stride(0), b_q.stride(1),\n        scales.stride(0), scales.stride(1),\n        zeros.stride(0), zeros.stride(1),\n        out.stride(0), out.stride(1),\n        group_size,\n        SPLIT_K=1,\n    )\n    return out.to(orig_device)\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `_fwd_kernel_flash_decode_stage2` Triton kernel is a parallel computation designed for processing sequences in a neural network context, specifically dealing with batches, heads, and sequence blocks. This kernel receives several inputs: `B_Seqlen`, `Mid_O`, `Mid_O_LogExpSum`, and `Out`, along with strides for indexing. `B_Seqlen` contains sequence lengths per batch, `Mid_O` contains intermediate outputs, `Mid_O_LogExpSum` holds log-exp sum values, and `Out` will store the final output. The kernel operates over a 2D grid defined by batch size and head count (`grid = (batch, head_num)`), with constants `BLOCK_SEQ` and `BLOCK_DMODEL` indicating sequence block size and dimension alignment respectively.\n\n            The kernel function operates as follows:\n            - Identifies the current batch and head using `tl.program_id`.\n            - Initializes accumulators: `sum_exp`, `max_logic`, and `acc` to accumulate exponential logic and values.\n            - Loads the current sequence length and calculates the number of sequence blocks (`block_n_size`).\n            - Iterates over each block, where:\n                - It loads values (`tv`) from `Mid_O` and logic sums (`tlogic`) from `Mid_O_LogExpSum`.\n                - Computes the maximum logic value across blocks and scales previous accumulations.\n                - Updates the accumulators by computing the exponential of adjusted logic values and scaling/accumulating.\n            - Stores the final normalized result into `Out`, scaling accumulated values by the sum of exponentials.\n\n            The `flash_decode_stage2` function sets up and invokes this kernel, determining dimensions and grid setup based on input tensor shapes. It ensures efficient computation by using Triton's parallel execution framework, specifying warp and stage numbers.\n            ", "label": null, "file": "flash_decode2_phi.py", "test_code": "\n\n\nimport torch\n\n# Define the test function\ndef test_flash_decode_stage2():\n    # Define the parameters for different test cases\n    batch_size = 2\n    head_num = 4\n    seq_block_num = 3\n    head_dim = 64\n    block_seq = 16\n\n    test_cases = {\n        \"test_case_1\": {\n            \"B_Seqlen\": torch.randint(1, seq_block_num * block_seq, (batch_size,), dtype=torch.int32, device='cuda'),\n            \"mid_out\": torch.randn((batch_size, head_num, seq_block_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size, head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n            \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"block_seq\": block_seq\n        },\n        \"test_case_2\": {\n            \"B_Seqlen\": torch.randint(1, seq_block_num * block_seq, (batch_size,), dtype=torch.int32, device='cuda'),\n            \"mid_out\": torch.randn((batch_size, head_num, seq_block_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size, head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n            \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"block_seq\": block_seq + 1  # Different block size\n        },\n        \"test_case_3\": {\n            \"B_Seqlen\": torch.randint(1, seq_block_num * block_seq, (batch_size,), dtype=torch.int32, device='cuda'),\n            \"mid_out\": torch.randn((batch_size, head_num, seq_block_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size, head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n            \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"block_seq\": block_seq // 2  # Different block size\n        },\n        \"test_case_4\": {\n            \"B_Seqlen\": torch.randint(1, seq_block_num * block_seq, (batch_size,), dtype=torch.int32, device='cuda'),\n            \"mid_out\": torch.randn((batch_size, head_num, seq_block_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size, head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n            \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32, device='cuda'),\n            \"block_seq\": block_seq * 2  # Different block size\n        }\n    }\n\n    # Execute the function for all test cases\n    results = {}\n    for key, test_case in test_cases.items():\n        flash_decode_stage2(test_case[\"mid_out\"], test_case[\"mid_out_logexpsum\"], test_case[\"B_Seqlen\"], test_case[\"Out\"], test_case[\"block_seq\"])\n        results[key] = test_case[\"Out\"]\n\n    return results\n\n# Run the test\nresult_gold = test_flash_decode_stage2()\n", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, Out,\n                                    stride_seqlen,\n                                    stride_mo_bh, stride_mo_s, stride_mo_d,\n                                    stride_mo_log_bh, stride_mo_log_s,\n                                    stride_out_bh, stride_out_d,\n                                    BLOCK_SEQ: tl.constexpr,\n                                    BLOCK_DMODEL: tl.constexpr,\n                                    SEQ_BLK_MAX: tl.constexpr):\n    pid_b = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    cur_seqlen = tl.load(B_Seqlen + pid_b * stride_seqlen)\n    block_n_size = (cur_seqlen + BLOCK_SEQ - 1) // BLOCK_SEQ\n\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    for blk in tl.static_range(SEQ_BLK_MAX):\n        if blk < block_n_size:\n            offs_mo_bh = pid_b * stride_mo_bh + pid_h\n            tv = tl.load(Mid_O + offs_mo_bh + blk * stride_mo_s + offs_d * stride_mo_d,\n                         mask=offs_d < BLOCK_DMODEL, other=0.0)\n            tlogic = tl.load(Mid_O_LogExpSum + offs_mo_bh * stride_mo_log_bh +\n                             blk * stride_mo_log_s)\n\n            new_max = tl.maximum(max_logic, tlogic)\n            scale_prev = tl.exp(max_logic - new_max)\n            scale_new = tl.exp(tlogic - new_max)\n            acc = acc * scale_prev + tv * scale_new\n            sum_exp = sum_exp * scale_prev + scale_new\n            max_logic = new_max\n\n    eps = 1e-6\n    if block_n_size > 0:\n        inv_sum = 1.0 / (sum_exp + eps)\n    else:\n        inv_sum = 0.0\n    final = acc * inv_sum\n\n    offs_out = pid_b * stride_out_bh + pid_h\n    tl.store(Out + offs_out + offs_d * stride_out_d,\n             final.to(Out.type.element_ty),\n             mask=offs_d < BLOCK_DMODEL)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4, num_stages=2),\n        triton.Config({}, num_warps=8, num_stages=2),\n    ],\n    key=['max_seqlen', 'head_num', 'head_dim'],\n)\ndef flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, Out,\n                        max_seqlen: int, head_num: int, head_dim: int):\n    assert B_Seqlen.dtype == torch.int32\n    batch = B_Seqlen.numel()\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n    SEQ_BLK_MAX = (max_seqlen + 63) // 64\n\n    grid = (batch, head_num)\n\n    _fwd_kernel_flash_decode_stage2[grid](\n        B_Seqlen, Mid_O, Mid_O_LogExpSum, Out,\n        B_Seqlen.stride(0),\n        Mid_O.stride(0), Mid_O.stride(2), Mid_O.stride(3),\n        Mid_O_LogExpSum.stride(0), Mid_O_LogExpSum.stride(2),\n        Out.stride(0), Out.stride(2),\n        BLOCK_SEQ=64,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        SEQ_BLK_MAX=SEQ_BLK_MAX\n    )\n\n    return Out\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton operator is defined to transpose a matrix using a kernel function and a wrapper function. The kernel function named 'kernel' is decorated with '@triton.jit' for just-in-time compilation and performs matrix transposition by directly manipulating pointers based on the given strides and dimensions. It accepts input parameters such as a matrix 'M', an output buffer 'Out', the strides of 'M' and 'Out', and the dimensions 'SIZE_M' and 'D_HEAD'. The kernel computes the pointers for elements of 'M' using 'matrix_stridex' and 'matrix_stridey', and for 'Out' using 'out_stridex' and 'out_stridey'. The transposition is achieved by loading elements from 'M' and storing them into 'Out' in a transposed layout. The wrapper function named 'wrapper' initializes 'matrix' with random float16 values and 'out' with zeros, both on CUDA. It defines the grid configuration as a tuple with a single element, then calls the kernel with these matrices and their properties. Finally, it returns the transposed matrix 'out'.\n    ", "label": null, "file": "matrix_transpose.py", "test_code": "\n\n\nimport torch\n\ndef test_triton_vs_torch():\n    results = {}\n\n    # \u6d4b\u8bd5\u7528\u4f8b 1: \u57fa\u672c\u77e9\u9635\u8f6c\u7f6e (\u5c0f\u77e9\u9635)\n    size_m, d_head = 16, 16\n    out = wrapper(size_m, d_head)\n    results[\"test_case_1\"] = out.clone()\n\n    # \u6d4b\u8bd5\u7528\u4f8b 2: \u975e\u65b9\u5f62\u77e9\u9635\n    size_m, d_head = 32, 64\n    out = wrapper(size_m, d_head)\n    results[\"test_case_2\"] = out.clone()\n\n    return results\n\n\n# \u8fd0\u884c\u6d4b\u8bd5\nresult_gold = test_triton_vs_torch()\n# print(result_gold)", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef kernel(\n    M,\n    Out,\n    matrix_stridex,\n    matrix_stridey,\n    out_stridex,\n    out_stridey,\n    SIZE_M,\n    D_HEAD,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    mask_m = offs_m < SIZE_M\n    mask_n = offs_n < D_HEAD\n\n    idx_m = offs_m[:, None] * matrix_stridey + offs_n[None, :] * matrix_stridex\n    idx_o = offs_n[:, None] * out_stridey + offs_m[None, :] * out_stridex\n\n    tile = tl.load(M + idx_m, mask=mask_m[:, None] & mask_n[None, :], other=0.0)\n    tl.store(Out + idx_o, tile.T, mask=mask_n[:, None] & mask_m[None, :])\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_stages=1, num_warps=8),\n    ],\n    key=['SIZE_M', 'D_HEAD'],\n)\n@triton.jit\ndef kernel(\n    M,\n    Out,\n    matrix_stridex,\n    matrix_stridey,\n    out_stridex,\n    out_stridey,\n    SIZE_M,\n    D_HEAD,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    mask_m = offs_m < SIZE_M\n    mask_n = offs_n < D_HEAD\n\n    idx_m = offs_m[:, None] * matrix_stridey + offs_n[None, :] * matrix_stridex\n    idx_o = offs_n[:, None] * out_stridey + offs_m[None, :] * out_stridex\n\n    tile = tl.load(M + idx_m, mask=mask_m[:, None] & mask_n[None, :], other=0.0)\n    tl.store(Out + idx_o, tile.T, mask=mask_n[:, None] & mask_m[None, :])\n\ndef wrapper(SIZE_M: int = 512, D_HEAD: int = 128):\n    matrix = torch.randn((SIZE_M, D_HEAD), dtype=torch.float16, device='cuda')\n    out = torch.empty((D_HEAD, SIZE_M), dtype=torch.float16, device='cuda')\n\n    grid = lambda META: (\n        triton.cdiv(SIZE_M, META['BLOCK_M']),\n        triton.cdiv(D_HEAD, META['BLOCK_N']),\n    )\n\n    kernel[grid](\n        matrix,\n        out,\n        matrix.stride(1),\n        matrix.stride(0),\n        out.stride(1),\n        out.stride(0),\n        SIZE_M,\n        D_HEAD,\n    )\n\n    return out\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The `rotary_kernel` function is a Triton kernel that performs rotary position encoding on a tensor `X` using precomputed cosine (`COS`) and sine (`SIN`) matrices. It modifies or populates the output tensor `OUT` with the transformed data. The kernel accommodates both fixed and variable sequence lengths, controlled by the presence of `CU_SEQLENS`. The kernel handles interleaved and non-interleaved formats and allows for in-place transformations and conjugate computations if specified.\n\n        The kernel operates in a three-dimensional grid, processing batches (`pid_batch`), heads (`pid_head`), and sequences (`pid_m`). It calculates transformations by loading blocks of data and applying rotary transformations based on cosine and sine values. The key operations are tailored based on whether the data is interleaved or not, with conditional handling for conjugation using `CONJUGATE`.\n\n        The `apply_rotary` function acts as a high-level interface to the Triton kernel. It accepts the input tensor `x`, cosine and sine matrices, sequence length offsets, and optional cumulative sequence lengths (`cu_seqlens`). The function determines the execution grid and block sizes, aligning them with the input data shape and configuration. It initializes an output tensor, copying non-rotary parts of `x` if required. The function ensures that the kernel is called with appropriate arguments, matching the shape and type expectations set within the kernel logic. This design allows for efficient rotary transformations in transformer architectures.\n    ", "label": null, "file": "rotary_transform.py", "test_code": "\n\n\nimport torch\n\ndef test_apply_rotary():\n    results = {}\n    \n    # Test case 1: Basic test with fixed sequence length and no interleaving\n    batch, seqlen, nheads, headdim = 2, 128, 4, 64\n    rotary_dim = 32\n    x = torch.randn(batch, seqlen, nheads, headdim, device='cuda')\n    cos = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    sin = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    output = apply_rotary(x, cos, sin)\n    results['test_case_1'] = output.shape\n\n    # Test case 2: Variable length sequences with interleaving\n    total_seqlen, nheads, headdim = 256, 4, 64\n    batch = 3\n    cu_seqlens = torch.tensor([0, 100, 200, 256], device='cuda')\n    max_seqlen = 128\n    rotary_dim = 32\n    x = torch.randn(total_seqlen, nheads, headdim, device='cuda')\n    cos = torch.randn(max_seqlen, rotary_dim // 2, device='cuda')\n    sin = torch.randn(max_seqlen, rotary_dim // 2, device='cuda')\n    output = apply_rotary(x, cos, sin, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen, interleaved=True)\n    results['test_case_2'] = output.shape\n\n    # Test case 3: Conjugate flag enabled\n    batch, seqlen, nheads, headdim = 2, 128, 4, 64\n    rotary_dim = 32\n    x = torch.randn(batch, seqlen, nheads, headdim, device='cuda')\n    cos = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    sin = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    output = apply_rotary(x, cos, sin, conjugate=True)\n    results['test_case_3'] = output.shape\n\n    # Test case 4: Inplace operation\n    batch, seqlen, nheads, headdim = 2, 128, 4, 64\n    rotary_dim = 32\n    x = torch.randn(batch, seqlen, nheads, headdim, device='cuda')\n    cos = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    sin = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    output = apply_rotary(x, cos, sin, inplace=True)\n    results['test_case_4'] = output.shape\n\n    return results\n\nresult_gold = test_apply_rotary()\n", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 4, 'BLOCK_K': 32},  num_warps=2, stages=1),         # safe\n        triton.Config({'BLOCK_M': 8, 'BLOCK_K': 32},  num_warps=2, stages=1),\n        triton.Config({'BLOCK_M': 4, 'BLOCK_K': 64},  num_warps=4, stages=1),\n        triton.Config({'BLOCK_M': 8, 'BLOCK_K': 64},  num_warps=4, stages=1),\n        triton.Config({'BLOCK_M': 8, 'BLOCK_K': 128}, num_warps=4, stages=1),\n        triton.Config({'BLOCK_M': 8, 'BLOCK_K': 256}, num_warps=8, stages=1),         # aggressive\n    ],\n    key=['HEAD_DIM', 'INTERLEAVED'],\n)\n@triton.jit\ndef rotary_kernel(\n    X,\n    COS,\n    SIN,\n    OUT,\n    CU_SEQLENS,\n    SEQ_OFFSETS,\n    stride_xb,\n    stride_xh,\n    stride_xm,\n    stride_xk,\n    stride_cos_m,\n    stride_cos_k,\n    stride_sin_m,\n    stride_sin_k,\n    stride_ob,\n    stride_oh,\n    stride_om,\n    stride_ok,\n    nheads,\n    rotary_dim: tl.constexpr,\n    seqlen: tl.constexpr,\n    seqlen_log2: tl.constexpr,\n    interleaved: tl.constexpr,\n    conjugate: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    pid_head  = tl.program_id(2)\n\n    if CU_SEQLENS is not None:\n        seq_start = tl.load(CU_SEQLENS + pid_batch)\n        seq_end   = tl.load(CU_SEQLENS + pid_batch + 1)\n        cur_seqlen = seq_end - seq_start\n    else:\n        seq_start = 0\n        cur_seqlen = seqlen\n\n    if pid_m * BLOCK_M >= cur_seqlen:\n        return\n\n    actual_m = seq_start + pid_m * BLOCK_M\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rk = tl.arange(0, BLOCK_K)\n\n    offs_m = rm[:, None]\n    offs_k = rk[None, :]\n\n    x_ptr_base = X + (pid_batch * stride_xb) + (pid_head * stride_xh)\n    cos_ptr_base = COS + (actual_m * stride_cos_m)\n    sin_ptr_base = SIN + (actual_m * stride_sin_m)\n\n    mask_m = rm < cur_seqlen\n    mask_k = rk < rotary_dim\n\n    if rot_dim := rotary_dim % BLOCK_K:\n        mask_k = rk < rot_dim if rot_dim else mask_k\n\n    # load X tile: [BLOCK_M, BLOCK_K]\n    x_tile = tl.load(x_ptr_base + offs_m * stride_xm + offs_k * stride_xk, mask=(mask_m[:, None] & mask_k[None, :]), other=0.0)\n\n    cos = tl.load(cos_ptr_base + rk * stride_cos_k, mask=mask_k, other=1.0)\n    sin = tl.load(sin_ptr_base + rk * stride_sin_k, mask=mask_k, other=0.0)\n\n    if conjugate:\n        sin = -sin\n\n    if interleaved:\n        cos = cos[None, :]\n        sin = sin[None, :]\n\n        x0 = x_tile[:, 0::2]\n        x1 = x_tile[:, 1::2]\n\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n\n        out_tile = tl.zeros([BLOCK_M, BLOCK_K], dtype=x_tile.dtype)\n        out_tile = tl.where((rk[None, :] % 2 == 0), o0, out_tile)\n        out_tile = tl.where((rk[None, :] % 2 == 1), o1, out_tile)\n    else:\n        cos = cos[None, :]\n        sin = sin[None, :]\n\n        o0 = x_tile * cos\n        o1 = tl.cat([x_tile[:, rotary_dim // 2 :], x_tile[:, 0 : rotary_dim // 2]], 1)\n        if conjugate:\n            o1 = -o1\n        o = o0 + sin * o1\n        out_tile = o\n\n    # store back\n    out_base = OUT + (pid_batch * stride_ob) + (pid_head * stride_oh)\n    tl.store(out_base + offs_m * stride_om + offs_k * stride_ok, out_tile, mask=(mask_m[:, None] & mask_k[None, :]))\n\n    # copy non-rotary features unchanged (BLOCK_K stride)\n    if rotary_dim != stride_xk * BLOCK_K and not (interleaved and rotary_dim == stride_xk * BLOCK_K // 2):\n        rem_k = rotary_dim\n        off_k_rem = rem_k + rk\n        mask_k_rem = off_k_rem < stride_xk * BLOCK_K\n        x_tail = tl.load(x_ptr_base + offs_m * stride_xm + off_k_rem[None, :] * stride_xk,\n                         mask=(mask_m[:, None] & mask_k_rem[None, :]), other=0.0)\n        tl.store(out_base + offs_m * stride_om + off_k_rem[None, :] * stride_ok,\n                 x_tail, mask=(mask_m[:, None] & mask_k_rem[None, :]))\n\n\ndef apply_rotary(\n    x,\n    cos,\n    sin,\n    seqlen_offsets=None,\n    cu_seqlens=None,\n    rotary_dim=None,\n    interleaved=False,\n    conjugate=False,\n    out=None,\n):\n    \"\"\"\n    x:       [batch, nheads, seqlen, headdim] or [batch, seqlen, nheads, headdim]\n    cos:     [seqlen, rotary_dim//2]   precomputed\n    sin:     same as cos\n    \"\"\"\n    device = x.device\n    dtype = x.dtype\n    shape = x.shape\n    headdim = shape[-1]\n    rotary_dim = headdim if rotary_dim is None else rotary_dim\n    assert rotary_dim <= headdim\n    assert rotary_dim % 2 == 0, \"rotary_dim must be even\"\n\n    if len(shape) == 4:\n        batch, nheads, seqlen, headdim = shape\n        x = x.transpose(1, 2).contiguous()          # [batch, seqlen, nheads, headdim]\n        batch, seqlen, nheads, headdim = x.shape\n    else:\n        raise ValueError(\"Unsupported shape\")\n\n    # ensure contiguous\n    x = x.contiguous()\n    cos = cos.contiguous()\n    sin = sin.contiguous()\n\n    assert cos.shape == (seqlen, rotary_dim // 2)\n    assert sin.shape == (seqlen, rotary_dim // 2)\n\n    if out is None:\n        out = torch.empty_like(x)\n\n    assert out.device == device\n    assert out.dtype == dtype\n\n    seqlen_log2 = int(2**((seqlen).bit_length() - 1).bit_length()) if seqlen > 0 else 1\n\n    grid = lambda META: (\n        triton.cdiv(seqlen, META['BLOCK_M']),\n        batch,\n        nheads,\n    )\n\n    rotary_kernel[grid](\n        x,\n        cos,\n        sin,\n        out,\n        cu_seqlens,\n        seqlen_offsets,\n        stride_xb=x.stride(0),\n        stride_xh=x.stride(2),\n        stride_xm=x.stride(1),\n        stride_xk=x.stride(3),\n        stride_cos_m=cos.stride(0),\n        stride_cos_k=cos.stride(1),\n        stride_sin_m=sin.stride(0),\n        stride_sin_k=sin.stride(1),\n        stride_ob=out.stride(0),\n        stride_oh=out.stride(2),\n        stride_om=out.stride(1),\n        stride_ok=out.stride(3),\n        nheads=nheads,\n        HEAD_DIM=headdim,\n        seqlen=seqlen,\n        seqlen_log2=seqlen_log2,\n        INTERLEAVED=interleaved,\n        CONJUGATE=conjugate,\n    )\n    if len(shape) == 4:\n        out = out.transpose(1, 2).contiguous()\n    return out\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code implements a Triton kernel named `kernel_function`, which processes input data using block-wise operations. \n            The kernel takes pointers to input and output data (`x_ptr` and `output_ptr`), the total number of elements to process (`n_elements`), and a constant block size (`BLOCK_SIZE`). \n            Inside the kernel, each program instance calculates its starting point (`block_start`) and creates an `offsets` tensor for element indexing. \n            A mask ensures operations only occur on valid indices within the input bounds. The kernel loads data from `x_ptr`, computes the sine using `tl.math.sin`, and stores the result in `output_ptr`. \n            The `call_kernel` function prepares to execute the kernel by calculating the total number of elements (`n_elements`) and creates an output tensor. \n            It defines a grid configuration function using lambda to handle thread block calculations based on `BLOCK_SIZE`, ensuring the entire input is processed. \n            The kernel is then launched with the grid configuration, input, output, and element count.\n            ", "label": null, "file": "sin_kernel.py", "test_code": "\n\n\nimport torch\n\n# Function to test the Triton kernel\ndef test_call_kernel():\n    results = {}\n    \n    # Test case 1: Small input tensor\n    x1 = torch.tensor([0.0, 1.0, 2.0, 3.0], dtype=torch.float32).cuda()\n    output1 = call_kernel(x1)\n    results['test_case_1'] = output1\n    \n    # Test case 2: Larger input tensor\n    x2 = torch.linspace(0, 10, steps=1024, dtype=torch.float32).cuda()\n    output2 = call_kernel(x2)\n    results['test_case_2'] = output2\n\n    # Test case 3: Edge case with zero elements\n    x3 = torch.tensor([], dtype=torch.float32).cuda()\n    output3 = call_kernel(x3)\n    results['test_case_3'] = output3\n\n    # Test case 4: Input tensor with negative values\n    x4 = torch.tensor([-1.0, -2.0, -3.0, -4.0], dtype=torch.float32).cuda()\n    output4 = call_kernel(x4)\n    results['test_case_4'] = output4\n    \n    return results\n\n# Run the test function\nresult_gold = test_call_kernel()\n", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef kernel_function(\n    x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.sin(x)\n    tl.store(output_ptr + offsets, y, mask=mask)\n\n\ndef call_kernel(x: torch.Tensor):\n    assert x.is_contiguous()\n    output = torch.empty_like(x)\n    n_elements = x.numel()\n    grid = lambda META: (triton.cdiv(n_elements, META['BLOCK_SIZE']),)\n    kernel_function[grid](\n        x, output, n_elements,\n        BLOCK_SIZE=1024,\n    )\n    return output\n\n\ndef kernel_function_wrapper(x: torch.Tensor):\n    return call_kernel(x)\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The Triton kernel `_l2_norm_bwd_kernel` performs a backward pass operation for L2 normalization on a per-row basis. It receives pointers to input `X`, output gradient `DY`, and calculates the input gradient `DX`. Each row of the input is accessed using the `stride_x_row`. `BLOCK_N` determines the number of elements processed per block, set based on maximum allowable fused size and next power of 2 of `N`. Within the kernel, it computes the variance of the input slice, uses it to compute the reciprocal of the standard deviation (`rstd`), and then calculates `dx` using the formula `dy * rstd - tl.sum(dy * x) * (1 / (var+eps)) * rstd * x`. The result is conditionally stored in `DX` using masks. The `_l2_norm_bwd` function orchestrates this process, ensuring input tensors `x` and `dy` are properly reshaped and their strides configured for contiguity if necessary. If `N` exceeds `BLOCK_N`, an error is raised to prevent excessive feature dimensions. Finally, the kernel is launched over `M` rows of the reshaped tensors, and the output `dx` is reshaped back to the original input shape.\n    ", "label": null, "file": "l2_norm_bwd.py", "test_code": "\n\n\nimport torch\n\n# Test the backward L2 normalization\ndef test_l2_norm_bwd():\n    results = {}\n    \n    # Test case 1: Default case\n    x = torch.randn(4, 8, device='cuda', dtype=torch.float32)\n    dy = torch.randn(4, 8, device='cuda', dtype=torch.float32)\n    dx = _l2_norm_bwd(x, dy)\n    results['test_case_1'] = dx\n\n    # Test case 2: Different shape\n    x = torch.randn(2, 16, device='cuda', dtype=torch.float32)\n    dy = torch.randn(2, 16, device='cuda', dtype=torch.float32)\n    dx = _l2_norm_bwd(x, dy)\n    results['test_case_2'] = dx\n\n    # Test case 3: Larger tensor\n    x = torch.randn(8, 8, device='cuda', dtype=torch.float32)\n    dy = torch.randn(8, 8, device='cuda', dtype=torch.float32)\n    dx = _l2_norm_bwd(x, dy)\n    results['test_case_3'] = dx\n\n    # Test case 4: Edge case with small tensor\n    x = torch.randn(1, 8, device='cuda', dtype=torch.float32)\n    dy = torch.randn(1, 8, device='cuda', dtype=torch.float32)\n    dx = _l2_norm_bwd(x, dy)\n    results['test_case_4'] = dx\n\n    return results\n\n# Run the tests\nresult_gold = test_l2_norm_bwd()\n", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _l2_norm_bwd_kernel(\n    X, DY, DX,\n    M, N,\n    stride_x_row, stride_dy_row, stride_dx_row,\n    eps,\n    BLOCK_N: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n\n    x_ptr = X + pid * stride_x_row\n    dy_ptr = DY + pid * stride_dy_row\n    dx_ptr = DX + pid * stride_dx_row\n\n    # Normal variant: each vector FP16\u2192FP32 before products\n    x_vals = tl.load(x_ptr + cols * tl.constexpr(1), mask=mask, other=0.0).to(tl.float32)\n    dy_vals = tl.load(dy_ptr + cols * tl.constexpr(1), mask=mask, other=0.0).to(tl.float32)\n\n    x2 = x_vals * x_vals\n    var = tl.sum(x2, axis=0)  # scalar\n\n    r = tl.sqrt(var + eps)\n    rstd = 1.0 / r\n\n    dx_partial = dy_vals * rstd\n    cross = tl.sum(dy_vals * x_vals, axis=0)  # (dy\u00b7x) scalar\n    dx = dx_partial - (cross * (rstd * rstd * rstd)) * x_vals\n\n    tl.store(dx_ptr + cols * tl.constexpr(1), dx.to(DX.type.element_ty), mask=mask)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 256}, num_warps=2, num_stages=1),\n        triton.Config({'BLOCK_N': 512}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_N': 1024}, num_warps=8, num_stages=1),\n    ],\n    key=['N']\n)\n@triton.jit\ndef _l2_norm_bwd_kernel_auto(\n    X, DY, DX,\n    M, N,\n    stride_x_row, stride_dy_row, stride_dx_row,\n    eps,\n    BLOCK_N: tl.constexpr,\n):\n    _l2_norm_bwd_kernel(\n        X, DY, DX,\n        M, N,\n        stride_x_row, stride_dy_row, stride_dx_row,\n        eps,\n        BLOCK_N=BLOCK_N,\n    )\n\n\ndef _l2_norm_bwd(x, dy, eps: float = 1e-5):\n    assert x.shape == dy.shape\n    device = x.device\n    x = x.contiguous()\n    dy = dy.contiguous()\n    orig_shape = x.shape\n\n    if x.ndim == 2:\n        M, N = x.shape\n    elif x.ndim == 3:\n        M = x.size(0) * x.size(1)\n        N = x.size(2)\n        x = x.view(M, N)\n        dy = dy.view(M, N)\n    else:\n        M = 1\n        N = x.numel()\n        x = x.view(M, N)\n        dy = dy.view(M, N)\n\n    dx = torch.empty_like(x)\n\n    # Allocations OK\n    assert x.dtype in (torch.float16, torch.bfloat16, torch.float32)\n    assert dy.dtype == x.dtype\n    assert dx.dtype == x.dtype\n    assert x.is_contiguous()\n    assert dy.is_contiguous()\n\n   <|reserved_token_163799|> BAR\n    grid = lambda META: (triton.cdiv(M, 1),)\n\n    _l2_norm_bwd_kernel_auto[grid](\n        x, dy, dx,\n        M, N,\n        x.stride(-2) if x.ndim >= 2 else 1,\n        dy.stride(-2) if dy.ndim >= 2 else 1,\n        dx.stride(-2) if dx.ndim >= 2 else 1,\n        eps,\n    )\n\n    dx = dx.view(orig_shape)\n    return dx\n"}, {"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `_l2_norm_fwd_1pass_kernel` aims to perform L2 normalization on a 2D input tensor `X`. It processes each row separately using Triton's parallel execution model. The kernel expects pointers to `X` and `Y` along with the stride for rows (`stride_x_row`), number of columns in `X` (`N`), a small constant `eps` to prevent division by zero, and a compile-time constant `BLOCK_N`. The kernel computes L2 normalization by first loading a block of data from `X`, calculating the sum of squares for variance, and computing the reciprocal of the square root of the variance plus `eps` to get `rstd`. It then multiplies the input block by `rstd` to produce the normalized values, which are stored in `Y`.\n\n            The Python function `_l2_norm_fwd` handles the setup and execution of the kernel. It first reshapes and possibly makes the input tensor `x` contiguous. It initializes an empty tensor `y` to store the output. The function calculates `BLOCK_N` based on `x`'s element size and ensures it doesn't exceed 64KB. If the feature dimension `N` is larger than `BLOCK_N`, it raises an error. The kernel is then launched with the total number of rows `M`, pointers to `x` and `y`, stride, number of columns, `eps`, and `BLOCK_N`. Finally, the function returns the normalized tensor reshaped to its original dimensions.\n            ", "label": null, "file": "l2_norm_triton1.py", "test_code": "\n\n\nimport torch\n\n# Test the forward L2 normalization\ndef test_l2_norm_fwd():\n    results = {}\n    \n    # Test case 1\n    x1 = torch.randn(4, 8, device='cuda', dtype=torch.float32)\n    y1 = _l2_norm_fwd(x1)\n    results['test_case_1'] = y1\n\n    # Test case 2: Different batch size\n    x2 = torch.randn(2, 8, device='cuda', dtype=torch.float32)\n    y2 = _l2_norm_fwd(x2)\n    results['test_case_2'] = y2\n\n    # Test case 3: Different feature size\n    x3 = torch.randn(4, 4, device='cuda', dtype=torch.float32)\n    y3 = _l2_norm_fwd(x3)\n    results['test_case_3'] = y3\n\n    # Test case 4: Larger tensor\n    x4 = torch.randn(8, 8, device='cuda', dtype=torch.float32)\n    y4 = _l2_norm_fwd(x4)\n    results['test_case_4'] = y4\n\n    return results\n\nresult_gold = test_l2_norm_fwd()\n", "predict": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# =========================================\n# Triton 3.2+ kernel: _l2_norm_fwd_1pass_kernel\n# =========================================\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_N\": 256}, num_warps=2, num_stages=1),\n        triton.Config({\"BLOCK_N\": 512}, num_warps=4, num_stages=1),\n        triton.Config({\"BLOCK_N\": 1024}, num_warps=8, num_stages=1),\n        triton.Config({\"BLOCK_N\": 2048}, num_warps=16, num_stages=1),\n    ],\n    key=[],\n)\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(\n    X,\n    Y,\n    stride_x_row,\n    N,\n    eps,\n    BLOCK_N: tl.constexpr,\n):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n\n    # load row into outer dim\n    x_ptr = X + row * stride_x_row\n    x = tl.load(x_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n\n    # variance\n    var = tl.sum(x * x, axis=0)\n\n    # rstd\n    rstd = 1.0 / tl.sqrt(var + eps)\n\n    # normalize & write back\n    y = x * rstd\n    y_ptr = Y + row * N\n    tl.store(y_ptr + cols, y, mask=mask)\n\n\n# =========================================\n# Wrapper: _l2_norm_fwd\n# =========================================\ndef _l2_norm_fwd(x: torch.Tensor, eps: float = 1e-6):\n    # flat/reshape preserving contiguous-inner-row\n    shape = x.shape\n    if x.dim() > 2:\n        x = x.view(-1, shape[-1])\n    x = x.contiguous()\n\n    M, N = x.shape\n    elem_size = x.element_size()\n    BLOCK_N_MAX = 1024 * 64 // elem_size  # 64 KB limit\n\n    # Choose largest power-of-two \u2264 BLOCK_N_MAX & \u2265 N\n    p = 1\n    while p * 2 <= min(BLOCK_N_MAX, N):\n        p *= 2\n    if p < N:\n        raise RuntimeError(\"Feature dimension N too large for Triton single-pass kernel.\")\n\n    y = torch.empty_like(x, memory_format=torch.contiguous_format)\n\n    grid = (M,)\n    _l2_norm_fwd_1pass_kernel[grid](\n        x,\n        y,\n        x.stride(0),\n        N,\n        eps,\n        BLOCK_N=p,\n    )\n\n    return y.view(shape)\n"}]